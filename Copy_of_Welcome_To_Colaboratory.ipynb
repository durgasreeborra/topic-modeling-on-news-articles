{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durgasreeborra/topic-modeling-on-news-articles/blob/main/Copy_of_Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3WYEoTT20_r"
      },
      "source": [
        "#**Project Title : TOPIC MODELLING ON NEWS ARTICLES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW-xmzeb_pdy"
      },
      "source": [
        "#**Contribution-Team**\n",
        "\n",
        "\n",
        "**Team Member 1-Jashwanth Vemuri**\n",
        "\n",
        "**Team Member 2-Bhaskar purimitla**\n",
        "\n",
        "**Team Member 3-Meghana Kantamneni**\n",
        "\n",
        "**Team Member 4-Durgasree Borra**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x73MrCr61cr"
      },
      "source": [
        "#**Problem Description**:\n",
        "In this project your task is to identify major themes/topics across a collection of BBC news articles. You can\n",
        "use clustering algorithms such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA) etc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jZeo12H7hr-"
      },
      "source": [
        "#**Data Description:**\n",
        "The dataset contains a set of news articles for each major segment consisting of business, entertainment,\n",
        "politics, sports, and technology.\n",
        "\n",
        "\n",
        "\n",
        "**In this project, you are required to do.**\n",
        "\n",
        "• Create an aggregate dataset of all the news articles and perform topic modelling on the dataset.\n",
        "\n",
        "• Verify whether these topics correspond to the different tags available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEpNgoJ6wOaH"
      },
      "source": [
        "#**Meghana Workspace**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjOD-Hi896x1"
      },
      "source": [
        "**Connecting To Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA5v0KFUoJiO",
        "outputId": "b885d141-28fb-459c-8f35-8387a8897a7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wOxEACWz1i0"
      },
      "source": [
        "**Data Ingestion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "lX05t2Wtz53D",
        "outputId": "162d702e-676d-4337-c7eb-ec608d63f09b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a0b0a5f59001>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcategory_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/data/Temp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfile_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iso-8859-1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/data/Temp/business'"
          ]
        }
      ],
      "source": [
        "\n",
        "data = {}\n",
        "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
        "\n",
        "for category in categories:\n",
        "    category_data = []\n",
        "    category_dir = os.path.join(\"/content/drive/MyDrive/data/Temp\", category)\n",
        "\n",
        "    file_list = os.listdir(category_dir)\n",
        "    for file_name in file_list:\n",
        "        with open(os.path.join(category_dir, file_name), 'r', encoding='iso-8859-1') as file:\n",
        "            text = file.read()\n",
        "            category_data.append(text)\n",
        "    data[category] = category_data\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PJSmNNNA098"
      },
      "source": [
        "**Data Exploration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD0V5kZ2w5VS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Create an empty dictionary to store data categorized by different categories.\n",
        "data = {}\n",
        "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
        "\n",
        "# Define a dictionary to store exploration results.\n",
        "exploration_results = {}\n",
        "all_articles=[]\n",
        "\n",
        "# Loop through each category.\n",
        "for category in categories:\n",
        "    category_data = []\n",
        "    category_dir = os.path.join(\"/content/drive/MyDrive/data/Temp\", category)\n",
        "\n",
        "    file_list = os.listdir(category_dir)\n",
        "    for file_name in file_list:\n",
        "        with open(os.path.join(category_dir, file_name), 'r', encoding='iso-8859-1') as file:\n",
        "            text = file.read()\n",
        "            # Remove spaces between words\n",
        "            text = ' '.join(text.split())  # This removes extra spaces\n",
        "            category_data.append(text)\n",
        "\n",
        "    # Store the category's data in the 'data' dictionary.\n",
        "    data[category] = category_data\n",
        "    all_articles.extend(category_data)\n",
        "\n",
        "\n",
        "    # Perform data exploration for the current category.\n",
        "    num_documents = len(category_data)\n",
        "    total_characters = sum(len(doc) for doc in category_data)\n",
        "    average_characters = total_characters / num_documents\n",
        "\n",
        "    # Store exploration results in the 'exploration_results' dictionary.\n",
        "    exploration_results[category] = {\n",
        "        \"Num Documents\": num_documents,\n",
        "        \"Total Characters\": total_characters,\n",
        "        \"Average Characters per Document\": average_characters,\n",
        "    }\n",
        "# Task 2.1.1: Display the dataset's structure (first few rows)\n",
        "print(\"Task 2.1.1: Display the dataset's structure (first few rows)\")\n",
        "for category, category_data in data.items():\n",
        "    print(f\"Category: {category}\")\n",
        "    for i, document in enumerate(category_data[:5]):\n",
        "        print(f\"Document {i + 1}: {document[:100]}...\")  # Display the first 100 characters\n",
        "\n",
        "# Task 2.1.2: Print column names and data types\n",
        "print(\"\\nTask 2.1.2: Print column names and data types\")\n",
        "for category, category_data in data.items():\n",
        "    print(f\"Category: {category}\")\n",
        "    df = pd.DataFrame({'Text': category_data})\n",
        "    print(df.dtypes)\n",
        "    print()\n",
        "\n",
        "# Check for Missing Values:\n",
        "\n",
        "# Task 2.1.3: Identify missing values and provide a summary\n",
        "print(\"Task 2.1.3: Identify missing values and provide a summary\")\n",
        "for category, category_data in data.items():\n",
        "    print(f\"Category: {category}\")\n",
        "    df = pd.DataFrame({'Text': category_data})\n",
        "    missing_values = df.isnull().sum()\n",
        "    print(missing_values)\n",
        "    print()\n",
        "\n",
        "# Summary Statistics: (Not applicable to text data)\n",
        "\n",
        "# Task 2.1.4: Compute summary statistics for text lengths\n",
        "print(\"Task 2.1.4: Summary Statistics for Text Lengths\")\n",
        "for category, category_data in data.items():\n",
        "    print(f\"Category: {category}\")\n",
        "    df = pd.DataFrame({'Text': category_data})\n",
        "    df['Text Length'] = df['Text'].apply(len)\n",
        "    summary_stats = df['Text Length'].describe(percentiles=[.25, .75])\n",
        "    print(summary_stats[['count', 'mean', 'std', 'min', 'max']])\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Meyo_BHRxaYr"
      },
      "source": [
        "# **Durga sree work space**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrqZTek55Wew"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U1P4xZq5CLm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J1QT9Wd5IiK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikqIyboC5O92"
      },
      "source": [
        "# **JASHWANTH Workspace**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpxBft3T5b-l"
      },
      "source": [
        "# **Categorical Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3GY5eFk5aR3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from textblob import TextBlob  # For sentiment analysis\n",
        "\n",
        "# Create a function for category analysis\n",
        "def perform_category_analysis(category_data):\n",
        "    num_documents = len(category_data)\n",
        "    total_characters = sum(len(doc) for doc in category_data)\n",
        "    average_characters = total_characters / num_documents\n",
        "    return {\n",
        "        \"Num Documents\": num_documents,\n",
        "        \"Total Characters\": total_characters,\n",
        "        \"Average Characters per Document\": average_characters,\n",
        "    }\n",
        "\n",
        "# Initialize a list to store all article data\n",
        "all_articles = []\n",
        "\n",
        "# Loop through each category.\n",
        "for category in categories:\n",
        "    category_data = []\n",
        "    category_dir = os.path.join(\"/content/drive/MyDrive/data/Temp\", category)\n",
        "\n",
        "    file_list = os.listdir(category_dir)\n",
        "    for file_name in file_list:\n",
        "        with open(os.path.join(category_dir, file_name), 'r', encoding='iso-8859-1') as file:\n",
        "            text = file.read()\n",
        "            # Remove spaces between words\n",
        "            text = ' '.join(text.split())  # This removes extra spaces\n",
        "            category_data.append(text)\n",
        "\n",
        "    # Store the category's data in the 'data' dictionary.\n",
        "    data[category] = category_data\n",
        "\n",
        "    # Store all article data for later use\n",
        "    all_articles.extend(category_data)\n",
        "\n",
        "    # Perform data exploration for the current category.\n",
        "    exploration_results[category] = perform_category_analysis(category_data)\n",
        "\n",
        "# Calculate the total number of articles\n",
        "total_articles = len(all_articles)\n",
        "\n",
        "# Task 2.1.1: Display the dataset's structure (first few rows)\n",
        "print(\" Displaying the dataset's structure (first few rows)\")\n",
        "for category, category_data in data.items():\n",
        "    print(f\"Category: {category}\")\n",
        "    for i, document in enumerate(category_data[:5]):\n",
        "        print(f\"Document {i + 1}: {document[:100]}...\")  # Display the first 100 characters\n",
        "\n",
        "# Task 2.1.2: Print column names and data types\n",
        "print(\"\\n Column names and data types\")\n",
        "for category, category_data in data.items():\n",
        "    print(f\"Category: {category}\")\n",
        "    df = pd.DataFrame({'Text': category_data})\n",
        "    print(df.dtypes)\n",
        "    print()\n",
        "\n",
        "# Check for Missing Values:\n",
        "\n",
        "# Task 2.1.3: Identify missing values and provide a summary\n",
        "print(\"Identify missing values\")\n",
        "for category, category_data in data.items():\n",
        "    print(f\"Category: {category}\")\n",
        "    df = pd.DataFrame({'Text': category_data})\n",
        "    missing_values = df.isnull().sum()\n",
        "    print(missing_values)\n",
        "    print()\n",
        "\n",
        "\n",
        "# Task 2.1.4: Compute summary statistics for text lengths\n",
        "print(\"Task 2.1.4: Summary Statistics for Text Lengths\")\n",
        "for category, category_data in data.items():\n",
        "    print(f\"Category: {category}\")\n",
        "    df = pd.DataFrame({'Text': category_data})\n",
        "    df['Text Length'] = df['Text'].apply(len)\n",
        "    summary_stats = df['Text Length'].describe(percentiles=[.25, .75])\n",
        "    print(summary_stats[['count', 'mean', 'std', 'min', 'max']])\n",
        "    print()\n",
        "\n",
        "# Categorical Analysis\n",
        "print(\"Categorical Analysis\")\n",
        "\n",
        "# Calculate the category counts\n",
        "category_counts = [len(category_data) for category_data in data.values()]\n",
        "\n",
        "# Task 2.2.1: Category Distribution Visualization (Count)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=categories, y=category_counts)\n",
        "plt.title('Count of Articles in Each Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Calculate and display the category percentages\n",
        "category_percentages = [(category, len(category_data) / total_articles * 100) for category, category_data in data.items()]\n",
        "\n",
        "print(\"Category Percentages:\")\n",
        "for category, percentage in category_percentages:\n",
        "    print(f\"{category}: {percentage:.2f}%\")\n",
        "\n",
        "# Task 2.2.2: Category Distribution Visualization (Percentage)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=categories, y=[percentage for _, percentage in category_percentages])\n",
        "plt.title('Percentage of Articles in Each Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Percentage')\n",
        "\n",
        "# Task 2.3.2: Sentiment Analysis\n",
        "print(\"\\nSentiment Analysis\")\n",
        "for category, category_data in data.items():\n",
        "    print(f\"\\nCategory: {category}\")\n",
        "    sentiment_scores = [TextBlob(doc).sentiment.polarity for doc in category_data]\n",
        "\n",
        "    # Determine sentiment labels based on polarity\n",
        "    sentiment_labels = ['Positive' if score > 0 else 'Negative' if score < 0 else 'Neutral' for score in sentiment_scores]\n",
        "\n",
        "    # Create a DataFrame to store sentiment analysis results\n",
        "    sentiment_df = pd.DataFrame({'Sentiment Score': sentiment_scores, 'Sentiment Label': sentiment_labels})\n",
        "\n",
        "    # Display sentiment scores and labels\n",
        "    print(sentiment_df)\n",
        "\n",
        "\n",
        "# Task 2.4.1: Word Frequency Calculation and Visualization\n",
        "print(\"\\nWord Frequency Analysis and Visualization\")\n",
        "for category, category_data in data.items():\n",
        "    print(f\"\\nCategory: {category}\")\n",
        "    # Combine all documents in the category into a single text corpus\n",
        "    category_corpus = ' '.join(category_data)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = category_corpus.split()\n",
        "\n",
        "    # Calculate word frequencies\n",
        "    word_frequencies = pd.Series(words).value_counts().reset_index()\n",
        "    word_frequencies.columns = ['Word', 'Frequency']\n",
        "\n",
        "    # Display the top 10 most frequent words in the category\n",
        "    print(f\"Top 10 Most Frequent Words in {category} Category:\")\n",
        "    print(word_frequencies.head(10))\n",
        "\n",
        "    # Visualize the word frequency distribution for the top 20 words\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x='Word', y='Frequency', data=word_frequencies.head(20))\n",
        "    plt.title(f'Top 20 Most Frequent Words in {category} Category')\n",
        "    plt.xlabel('Word')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    # Interpretation\n",
        "    print(\"\\nInterpretation:\")\n",
        "    print(\"The word frequency analysis reveals the most frequent terms in the news articles for this category.\")\n",
        "    print(\"Some common words may include generic terms related to the category, but specific keywords may also appear.\")\n",
        "    print(\"These keywords could provide insights into the main topics or subjects covered in the articles.\")\n",
        "\n",
        "# Task 2.4.2: Word Cloud Visualization\n",
        "print(\"\\nWord Cloud Visualization\")\n",
        "for category, category_data in data.items():\n",
        "    print(f\"\\nCategory: {category}\")\n",
        "    # Combine all documents in the category into a single text corpus\n",
        "    category_corpus = ' '.join(category_data)\n",
        "\n",
        "    # Generate a word cloud from the text corpus\n",
        "    wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate(category_corpus)\n",
        "\n",
        "    # Display the word cloud\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(f'Word Cloud for {category} Category')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqRUO6Ux66-3"
      },
      "source": [
        "### **Category Distribution Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4KSIRjh7ZFb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=categories, y=[percentage for _, percentage in category_percentages])\n",
        "plt.title('Percentage of Articles in Each Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Percentage')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDGWKY8K7iIn"
      },
      "source": [
        "### **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAg3PGG67qLb"
      },
      "outputs": [],
      "source": [
        "for category, category_data in data.items():\n",
        "    print(f\"\\nCategory: {category}\")\n",
        "    sentiment_scores = [TextBlob(doc).sentiment.polarity for doc in category_data]\n",
        "\n",
        "    # Determine sentiment labels based on polarity\n",
        "    sentiment_labels = ['Positive' if score > 0 else 'Negative' if score < 0 else 'Neutral' for score in sentiment_scores]\n",
        "\n",
        "    # Create a DataFrame to store sentiment analysis results\n",
        "    sentiment_df = pd.DataFrame({'Sentiment Score': sentiment_scores, 'Sentiment Label': sentiment_labels})\n",
        "\n",
        "    # Display sentiment scores and labels\n",
        "    print(sentiment_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds4-WkHP7wPO"
      },
      "source": [
        "### **Word Frequency Calculation and Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc_lVJNq8lZD"
      },
      "outputs": [],
      "source": [
        "for category, category_data in data.items():\n",
        "    print(f\"\\nCategory: {category}\")\n",
        "    # Combine all documents in the category into a single text corpus\n",
        "    category_corpus = ' '.join(category_data)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = category_corpus.split()\n",
        "\n",
        "    # Calculate word frequencies\n",
        "    word_frequencies = pd.Series(words).value_counts().reset_index()\n",
        "    word_frequencies.columns = ['Word', 'Frequency']\n",
        "\n",
        "    # Display the top 10 most frequent words in the category\n",
        "    print(f\"Top 10 Most Frequent Words in {category} Category:\")\n",
        "    print(word_frequencies.head(10))\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x='Word', y='Frequency', data=word_frequencies.head(20))\n",
        "    plt.title(f'Top 20 Most Frequent Words in {category} Category')\n",
        "    plt.xlabel('Word')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auAzxnKI-qFX"
      },
      "source": [
        "### **Word Cloud Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvaqfC_w-7sn"
      },
      "outputs": [],
      "source": [
        "for category, category_data in data.items():\n",
        "    print(f\"\\nCategory: {category}\")\n",
        "    # Combine all documents in the category into a single text corpus\n",
        "    category_corpus = ' '.join(category_data)\n",
        "\n",
        "    # Generate a word cloud from the text corpus\n",
        "    wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate(category_corpus)\n",
        "\n",
        "    # Display the word cloud\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(f'Word Cloud for {category} Category')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6wRPSHr2STr"
      },
      "source": [
        "#**Bhaskar Workspace**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VR078WT3LE6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRfUNBkWQ6xr"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DzTAT49QtUd"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojrx3uOHRLUl"
      },
      "outputs": [],
      "source": [
        "def tokens(text):\n",
        "    tokenization=word_tokenize(text)\n",
        "    return ' '.join(i for i in tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT1zgs1qRRJQ"
      },
      "source": [
        "**Punctuation Removing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJRsblCxRZKJ"
      },
      "outputs": [],
      "source": [
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWke2-vTR2FN"
      },
      "outputs": [],
      "source": [
        "def punctuation_removal(text):\n",
        "    return ''.join(word for word in text if word not in string.punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6P2PXKOR7hI"
      },
      "source": [
        "**Removing Stop words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqecXFhqHRAB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrAkbbEGSB_J"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5xVCcWDSJNG"
      },
      "outputs": [],
      "source": [
        "#view stopwords in 'english' language\n",
        "print(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvyj-CTASatD"
      },
      "outputs": [],
      "source": [
        "def stop(text):\n",
        "    stop_word=stopwords.words('english')\n",
        "    without_stopwords=[]\n",
        "    for i in text.split():\n",
        "        if i not in stop_word:\n",
        "            without_stopwords.append(i)\n",
        "    return ' '.join(i for i in without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7tmL3RRS_P7"
      },
      "source": [
        "**Performing pre-processing on our data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQPAqorHKIiR"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuVUcXZUTkLL"
      },
      "outputs": [],
      "source": [
        "#Accessing the each file from the dictionary\n",
        "for value in data.values():\n",
        "    cleaned_data=[]\n",
        "    for text in value:\n",
        "        # Removing punctuation and converting into lowercase\n",
        "        punct=punctuation_removal(text.lower())\n",
        "        # removing stopwords\n",
        "        new_words=stop(punct)\n",
        "        # tokenization and adding result to the list\n",
        "        cleaned_data.append(tokens(new_words))\n",
        "cleaned_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amKOCEBpVaNB"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rldyubFcVdir"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import LancasterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUu8q0uGViRU"
      },
      "outputs": [],
      "source": [
        "# initializing the LancasterStemmer class\n",
        "stemmer=LancasterStemmer()\n",
        "\n",
        "#create a function for stemming\n",
        "def st(text):\n",
        "    stemmed_words=[stemmer.stem(word) for word in text.split(' ')]\n",
        "    return ' '.join(i for i in stemmed_words)\n",
        "\n",
        "#create a new list to store the stemmed words\n",
        "result_text=[]\n",
        "for word in cleaned_data:\n",
        "    result_text.append(st(word))\n",
        "result_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kUpoeSZV06_"
      },
      "source": [
        "# **Vectorization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3OKFyDxV_us"
      },
      "source": [
        "**Tfidf-Vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09LXZQY5WG9V"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql1cLd_ZWNvN"
      },
      "outputs": [],
      "source": [
        "#initializing the Tfidf-Vectorizer Class\n",
        "vectorizer=TfidfVectorizer(lowercase=True,stop_words='english',max_features=100)\n",
        "\n",
        "# fit and transform the data into the vectorizer\n",
        "tfidf_matrix=vectorizer.fit_transform(result_text)\n",
        "#convert matrix into array\n",
        "tfidf_array=tfidf_matrix.toarray()\n",
        "\n",
        "#convert array into\n",
        "tfidf_dataframe=pd.DataFrame(tfidf_array)\n",
        "tfidf_dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Xn-ebrWTxk"
      },
      "source": [
        "**CountVectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu4ncsw5WYrq"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "emj0Ka60Wc6W",
        "outputId": "812bd988-5efd-4c7b-baaa-ef462b748205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-450e526c2941>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# intializing the Countvectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#fit the data into the vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
          ]
        }
      ],
      "source": [
        "# intializing the Countvectorizer\n",
        "cv=CountVectorizer(max_features=20)\n",
        "\n",
        "#fit the data into the vectorizer\n",
        "matrix=cv.fit_transform(result_text)\n",
        "\n",
        "#converting the matrix into an array\n",
        "array=matrix.toarray()\n",
        "\n",
        "#converting array into dataframe\n",
        "countvect_dataframe=pd.DataFrame(array)\n",
        "countvect_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the Tfidf-Vectorizer Class with adjusted parameters\n",
        "vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', max_features=300, min_df=2, max_df=0.95)\n",
        "\n",
        "# Fit and transform the data into the vectorizer\n",
        "tfidf_matrix = vectorizer.fit_transform(result_text)\n",
        "\n",
        "# Rest of the code remains the same for generating word clouds.\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Map TF-IDF values to words\n",
        "tfidf_words = [feature_names[i] for i in range(tfidf_matrix.shape[1])]\n",
        "\n",
        "# Create a dictionary that associates words with their TF-IDF values\n",
        "word_tfidf_dict = dict(zip(tfidf_words, tfidf_matrix.toarray()[0]))\n",
        "\n",
        "# Generate the word cloud from the word-TFIDF dictionary\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_tfidf_dict)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title(\"Word Cloud for TF-IDF Data\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pGcx82Ky9Yr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the Tfidf-Vectorizer Class with adjusted parameters\n",
        "vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', max_features=300, min_df=2, max_df=0.95)\n",
        "\n",
        "# Fit and transform the data into the vectorizer\n",
        "tfidf_matrix = vectorizer.fit_transform(result_text)\n",
        "\n",
        "# Rest of the code remains the same for generating word clouds.\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Map TF-IDF values to words\n",
        "tfidf_words = [feature_names[i] for i in range(tfidf_matrix.shape[1])]\n",
        "\n",
        "# Create a dictionary that associates words with their TF-IDF values\n",
        "word_tfidf_dict = dict(zip(tfidf_words, tfidf_matrix.toarray()[0]))\n",
        "\n",
        "# Generate the word cloud from the word-TFIDF dictionary\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_tfidf_dict)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title(\"Word Cloud for TF-IDF Data\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LS88SkWw9kPj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}